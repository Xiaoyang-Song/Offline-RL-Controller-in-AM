{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d213d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813e0107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xiaoyangsong/Desktop/Offline-RL-Controller-in-AM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a0e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa26652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1053, 1) () () () 435.21418112695454 730.7743435678826 -0.36122621301829466\n",
      "(1053, 1) () () () 126.19661470414442 1421.1915508818497 -0.37706539554349666\n",
      "(1053, 1) () () () 489.385336080055 979.1369092305144 -3.486651536021735\n",
      "(1053, 1) () () () 504.76113742873747 1475.521122545086 -7.3816908121369105\n",
      "(1053, 1) () () () 152.88703918942826 1252.1645764000257 -3.3861466576663957\n",
      "(1053, 1) () () () 513.6671540129037 879.1502090256 -8.104272687881398\n",
      "(1053, 1) () () () 134.9943433746089 415.3986297303739 -3.9907598469614585\n",
      "(1053, 1) () () () 517.8947077982968 929.7149581288493 -9.836594728975825\n"
     ]
    }
   ],
   "source": [
    "def extract_single_trajectory(trajectory_id, trajectory_length=8):\n",
    "    trajectory = []\n",
    "    for j in range(trajectory_length):\n",
    "        filename = os.path.join('..', 'RL Controller', 'RL_Dataset', f'trajectory_00{trajectory_id}', f'layer_{j+1}_data.mat')\n",
    "        data = loadmat(filename)\n",
    "        # Extract s, a, r\n",
    "        ss = data['SS_action'][0][0]\n",
    "        lp = data['LP_action'][0][0]\n",
    "        u = np.array(data['uFinal'])\n",
    "        uAll = data['uAll']\n",
    "        r = -data['meanDeviation'][0][0]\n",
    "        # Append\n",
    "        sar = [u, lp, r]\n",
    "        print(u.shape, lp.shape, ss.shape, r.shape, lp, ss, r)\n",
    "        trajectory.append(sar)\n",
    "    return trajectory\n",
    "\n",
    "trajectory = extract_single_trajectory(trajectory_id=1, trajectory_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b7565c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1053, 1) () () () 435.21418112695454 730.7743435678826 -0.36122621301829466\n",
      "(1053, 1) () () () 126.19661470414442 1421.1915508818497 -0.37706539554349666\n",
      "(1053, 1) () () () 489.385336080055 979.1369092305144 -3.486651536021735\n",
      "(1053, 1) () () () 504.76113742873747 1475.521122545086 -7.3816908121369105\n",
      "(1053, 1) () () () 152.88703918942826 1252.1645764000257 -3.3861466576663957\n",
      "(1053, 1) () () () 513.6671540129037 879.1502090256 -8.104272687881398\n",
      "(1053, 1) () () () 134.9943433746089 415.3986297303739 -3.9907598469614585\n",
      "(1053, 1) () () () 517.8947077982968 929.7149581288493 -9.836594728975825\n",
      "(1053, 1) () () () 301.77164668602336 581.0718911787624 -0.5544212187870767\n",
      "(1053, 1) () () () 181.71106387504352 1352.0131111735564 -0.36400716823206797\n",
      "(1053, 1) () () () 175.8699196795555 1300.2377529680389 -0.18720062542809024\n",
      "(1053, 1) () () () 312.40573743068705 904.7208555560082 -1.9851228930730056\n",
      "(1053, 1) () () () 498.11162169650294 580.5579760932429 -6.678266150879865\n",
      "(1053, 1) () () () 516.5121700490793 936.634244116746 -9.213721707748533\n",
      "(1053, 1) () () () 505.40177947835116 683.1587835622468 -11.771215866506177\n",
      "(1053, 1) () () () 445.9532814969507 912.21793317129 -12.123351110280357\n",
      "(1053, 1) () () () 489.212728549474 529.7885005473782 -0.9650225881112601\n",
      "(1053, 1) () () () 333.90196088811194 969.8707617054902 -1.9172354522195545\n",
      "(1053, 1) () () () 345.2942661168064 850.496989595545 -2.911352538206123\n",
      "(1053, 1) () () () 446.315112911064 1109.0135077700763 -6.324672566790493\n",
      "(1053, 1) () () () 558.869127482727 899.2947075322661 -9.999343625591843\n",
      "(1053, 1) () () () 544.9131365627225 1438.0028206731197 -11.54756760861093\n",
      "(1053, 1) () () () 258.56196714463806 1422.054483129365 -8.333716851697211\n",
      "(1053, 1) () () () 392.54260648515503 1280.896587834156 -9.716728038689375\n"
     ]
    }
   ],
   "source": [
    "def gather_dataset(id_list, trajectory_length=8):\n",
    "    dataset = []\n",
    "    for trajectory_id in id_list:\n",
    "        traj = extract_single_trajectory(trajectory_id, trajectory_length)\n",
    "        dataset.append(traj)\n",
    "    return dataset\n",
    "\n",
    "id_list = [1, 2, 3]\n",
    "dataset = gather_dataset(id_list, trajectory_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e884b0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4278ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ============================\n",
    "# Q-network\n",
    "# ============================\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + 1, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Policy network: a = π(s)\n",
    "# ============================\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.net(s)\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# Dataset loader from list-of-trajectories\n",
    "# =============================================\n",
    "def flatten_dataset(trajectories):\n",
    "    states, actions, rewards, next_states = [], [], [], []\n",
    "\n",
    "    for traj in trajectories:\n",
    "        for t in range(len(traj) - 1):\n",
    "            s, a, r = traj[t]\n",
    "            s2, _, _ = traj[t+1]\n",
    "\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            next_states.append(s2)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(np.array(states), dtype=torch.float32).to(device),\n",
    "        torch.tensor(np.array(actions), dtype=torch.float32).unsqueeze(-1).to(device),\n",
    "        torch.tensor(np.array(rewards), dtype=torch.float32).unsqueeze(-1).to(device),\n",
    "        torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# Offline Q-Learning loop (Fitted Q Iteration)\n",
    "# =============================================\n",
    "def train_fqi(trajectories, state_dim, gamma=0.99, K=50):\n",
    "    # Load flattened dataset\n",
    "    S, A, R, S2 = flatten_dataset(trajectories)\n",
    "    S = S.squeeze()\n",
    "    S2 = S2.squeeze()\n",
    "\n",
    "    qnet = QNet(state_dim).to(device)\n",
    "    policy = PolicyNet(state_dim).to(device)\n",
    "\n",
    "    q_opt = optim.Adam(qnet.parameters(), lr=1e-3)\n",
    "    p_opt = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "\n",
    "    for k in range(K):\n",
    "        # ----------------------\n",
    "        # Policy Improvement: update π so that it maximizes Q\n",
    "        # ----------------------\n",
    "        p_opt.zero_grad()\n",
    "        a_hat = policy(S)                  # predicted continuous actions\n",
    "        q_val = qnet(S, a_hat)\n",
    "        loss_policy = -q_val.mean()        # maximize Q => minimize -Q\n",
    "        loss_policy.backward()\n",
    "        p_opt.step()\n",
    "\n",
    "        # ----------------------\n",
    "        # Q-iteration step\n",
    "        # ----------------------\n",
    "        with torch.no_grad():\n",
    "            next_action = policy(S2)\n",
    "            target = R + gamma * qnet(S2, next_action)\n",
    "\n",
    "        q_opt.zero_grad()\n",
    "        q_pred = qnet(S, A)\n",
    "        loss_q = nn.MSELoss()(q_pred, target)\n",
    "        loss_q.backward()\n",
    "        q_opt.step()\n",
    "\n",
    "        print(f\"Iter {k}:  Q Loss={loss_q.item():.4f},  Policy Loss={loss_policy.item():.4f}\")\n",
    "\n",
    "    return qnet, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cc6f96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0:  Q Loss=1095.5637,  Policy Loss=111.6001\n",
      "Iter 1:  Q Loss=662120.6875,  Policy Loss=3234.4185\n",
      "Iter 2:  Q Loss=2838951.2500,  Policy Loss=6800.5698\n",
      "Iter 3:  Q Loss=8134104.5000,  Policy Loss=11667.6484\n",
      "Iter 4:  Q Loss=18853762.0000,  Policy Loss=17920.1465\n",
      "Iter 5:  Q Loss=38560360.0000,  Policy Loss=25776.9375\n",
      "Iter 6:  Q Loss=72388072.0000,  Policy Loss=35478.0039\n",
      "Iter 7:  Q Loss=127500544.0000,  Policy Loss=47260.9844\n",
      "Iter 8:  Q Loss=213547568.0000,  Policy Loss=61347.5430\n",
      "Iter 9:  Q Loss=343456928.0000,  Policy Loss=77997.5625\n",
      "Iter 10:  Q Loss=534244704.0000,  Policy Loss=97491.4219\n",
      "Iter 11:  Q Loss=808068544.0000,  Policy Loss=120132.7656\n",
      "Iter 12:  Q Loss=1193498752.0000,  Policy Loss=146250.5781\n",
      "Iter 13:  Q Loss=1727051520.0000,  Policy Loss=176200.0781\n",
      "Iter 14:  Q Loss=2454903040.0000,  Policy Loss=210363.4219\n",
      "Iter 15:  Q Loss=3434707456.0000,  Policy Loss=249149.7656\n",
      "Iter 16:  Q Loss=4738453504.0000,  Policy Loss=292995.2812\n",
      "Iter 17:  Q Loss=6455211520.0000,  Policy Loss=342369.1562\n",
      "Iter 18:  Q Loss=8694511616.0000,  Policy Loss=397771.0000\n",
      "Iter 19:  Q Loss=11590346752.0000,  Policy Loss=459732.3750\n",
      "Iter 20:  Q Loss=15306366976.0000,  Policy Loss=528818.8750\n",
      "Iter 21:  Q Loss=20038631424.0000,  Policy Loss=605632.1875\n",
      "Iter 22:  Q Loss=26024443904.0000,  Policy Loss=690804.5000\n",
      "Iter 23:  Q Loss=33547175936.0000,  Policy Loss=785007.6250\n",
      "Iter 24:  Q Loss=42946297856.0000,  Policy Loss=888950.1250\n",
      "Iter 25:  Q Loss=54622326784.0000,  Policy Loss=1003380.1875\n",
      "Iter 26:  Q Loss=69050023936.0000,  Policy Loss=1129081.6250\n",
      "Iter 27:  Q Loss=86788554752.0000,  Policy Loss=1266881.1250\n",
      "Iter 28:  Q Loss=108493963264.0000,  Policy Loss=1417645.7500\n",
      "Iter 29:  Q Loss=134933356544.0000,  Policy Loss=1582285.0000\n",
      "Iter 30:  Q Loss=167000276992.0000,  Policy Loss=1761750.6250\n",
      "Iter 31:  Q Loss=205732577280.0000,  Policy Loss=1957037.0000\n",
      "Iter 32:  Q Loss=252331048960.0000,  Policy Loss=2169182.7500\n",
      "Iter 33:  Q Loss=308181270528.0000,  Policy Loss=2399271.2500\n",
      "Iter 34:  Q Loss=374876995584.0000,  Policy Loss=2648429.7500\n",
      "Iter 35:  Q Loss=454245810176.0000,  Policy Loss=2917831.2500\n",
      "Iter 36:  Q Loss=548377919488.0000,  Policy Loss=3208694.7500\n",
      "Iter 37:  Q Loss=659656540160.0000,  Policy Loss=3522285.7500\n",
      "Iter 38:  Q Loss=790792437760.0000,  Policy Loss=3859915.5000\n",
      "Iter 39:  Q Loss=944859447296.0000,  Policy Loss=4222943.0000\n",
      "Iter 40:  Q Loss=1125336088576.0000,  Policy Loss=4612773.0000\n",
      "Iter 41:  Q Loss=1336149278720.0000,  Policy Loss=5030860.0000\n",
      "Iter 42:  Q Loss=1581720010752.0000,  Policy Loss=5478704.0000\n",
      "Iter 43:  Q Loss=1867013292032.0000,  Policy Loss=5957854.0000\n",
      "Iter 44:  Q Loss=2197595750400.0000,  Policy Loss=6469905.5000\n",
      "Iter 45:  Q Loss=2579687014400.0000,  Policy Loss=7016503.0000\n",
      "Iter 46:  Q Loss=3020256706560.0000,  Policy Loss=7599345.0000\n",
      "Iter 47:  Q Loss=3526993903616.0000,  Policy Loss=8220156.0000\n",
      "Iter 48:  Q Loss=4108507414528.0000,  Policy Loss=8880733.0000\n",
      "Iter 49:  Q Loss=4774320144384.0000,  Policy Loss=9582904.0000\n"
     ]
    }
   ],
   "source": [
    "state_dim = 1053\n",
    "qnet, policy = train_fqi(dataset, state_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1bdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
